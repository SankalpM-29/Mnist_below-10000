{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abbad5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0173ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "   tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba6b8f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4001bae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are downloading the MNIST dataset and splitting the data for training and testing\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ab5dd5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fed2e2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cd47153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1519d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping our training and testing datatset using numpy's reshape function which we will feed to the model\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6283f902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c49e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing type conversion or changing the datatype to float32 for the data\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "#Doing standardization or normalization here dividind each pixel by 255 in the train and test data\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62277e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Checking first 10 image labels\n",
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9c3b8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
    "# simply we can say we are doing sort of onehot encoding\n",
    "Y_train = tensorflow.keras.utils.to_categorical(y_train, 10)\n",
    "Y_test = tensorflow.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f767563a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# having a look in the first 10 datapoints after onehot encoding\n",
    "Y_train[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47594cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "# importing different layers and activations from keras.layers\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Add\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f9c979f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 26, 26, 8)         80        \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 26, 26, 8)         32        \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 26, 26, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 24, 24, 8)         584       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 24, 24, 8)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 11, 11, 10)        330       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 11, 11, 10)        40        \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 11, 11, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 9, 9, 12)          1092      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 9, 9, 12)          48        \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 9, 9, 12)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 7, 7, 12)          1308      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 7, 7, 12)          48        \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 7, 7, 12)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 6, 6, 12)          588       \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 6, 6, 12)          48        \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 6, 6, 12)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 4, 4, 16)          1744      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 4, 4, 16)          64        \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 1, 1, 10)          2570      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 8,576\n",
      "Trainable params: 8,436\n",
      "Non-trainable params: 140\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# importing Activation, BatchNormalization and MaxPooling2D from tensorflow.keras.layers for performing maxpooling and batchnormalizing operations and adding non linearity via activation functions\n",
    "from tensorflow.keras.layers import Activation,BatchNormalization\n",
    "\n",
    "# building our sequential model using the Sequential class and creating the model object\n",
    "model = Sequential()\n",
    "\n",
    "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
    "model.add(Conv2D(8, (3, 3), activation='relu', input_shape=(28,28,1))) #Output Dim = 26x26x8     \n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Performing 2dconvolution followed by BatchNormalization and Dropout        \n",
    "model.add(Conv2D(8, (3, 3), activation='relu'))          # 24x24                  \n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))      # 12x12\n",
    "\n",
    "model.add(Conv2D(10, (2, 2), activation='relu')) # 11x11                     \n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(12, (3, 3), activation='relu'))    # 9x9                  \n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(12, (3, 3), activation='relu'))     # 7x7                 \n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(12, (2, 2), activation='relu'))   # 6x6                         \n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), activation='relu'))   # 4x4                      \n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Performing only 2dconvolution at the last convolution layer(no batchnormalization and dropout)\n",
    "model.add(Conv2D(10, (4, 4)))                                           # using 4x4 kernel to see the complete image\n",
    "\n",
    "# Here we are Flateening our dat i.e making it one dimensional which we will feed to the network.\n",
    "model.add(Flatten())\n",
    "#Using softmax activation function at the last layer which is used for multi class classification\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ab485e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.003.\n",
      "Epoch 1/30\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.1841 - accuracy: 0.9423 - val_loss: 0.0695 - val_accuracy: 0.9774\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0022744503.\n",
      "Epoch 2/30\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.0859 - accuracy: 0.9740 - val_loss: 0.0391 - val_accuracy: 0.9874\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0018315018.\n",
      "Epoch 3/30\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.0664 - accuracy: 0.9792 - val_loss: 0.0332 - val_accuracy: 0.9893\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0015329586.\n",
      "Epoch 4/30\n",
      "3750/3750 [==============================] - 21s 6ms/step - loss: 0.0573 - accuracy: 0.9820 - val_loss: 0.0286 - val_accuracy: 0.9917\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0013181019.\n",
      "Epoch 5/30\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.0512 - accuracy: 0.9840 - val_loss: 0.0314 - val_accuracy: 0.9908\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0011560694.\n",
      "Epoch 6/30\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.0477 - accuracy: 0.9849 - val_loss: 0.0304 - val_accuracy: 0.9906\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0010295127.\n",
      "Epoch 7/30\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.0438 - accuracy: 0.9867 - val_loss: 0.0306 - val_accuracy: 0.9917\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0009279307.\n",
      "Epoch 8/30\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.0428 - accuracy: 0.9862 - val_loss: 0.0283 - val_accuracy: 0.9915\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0008445946.\n",
      "Epoch 9/30\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.0389 - accuracy: 0.9876 - val_loss: 0.0310 - val_accuracy: 0.9905\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0007749935.\n",
      "Epoch 10/30\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.0379 - accuracy: 0.9883 - val_loss: 0.0282 - val_accuracy: 0.9920\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0007159905.\n",
      "Epoch 11/30\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.0379 - accuracy: 0.9879 - val_loss: 0.0238 - val_accuracy: 0.9925\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.000665336.\n",
      "Epoch 12/30\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.0353 - accuracy: 0.9886 - val_loss: 0.0261 - val_accuracy: 0.9916\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0006213753.\n",
      "Epoch 13/30\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.0356 - accuracy: 0.9889 - val_loss: 0.0235 - val_accuracy: 0.9923\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0005828638.\n",
      "Epoch 14/30\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.0342 - accuracy: 0.9890 - val_loss: 0.0219 - val_accuracy: 0.9936\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0005488474.\n",
      "Epoch 15/30\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.0336 - accuracy: 0.9892 - val_loss: 0.0217 - val_accuracy: 0.9934\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0005185825.\n",
      "Epoch 16/30\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.0318 - accuracy: 0.9901 - val_loss: 0.0216 - val_accuracy: 0.9933\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.000491481.\n",
      "Epoch 17/30\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.0312 - accuracy: 0.9896 - val_loss: 0.0205 - val_accuracy: 0.9932\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0004670715.\n",
      "Epoch 18/30\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.0311 - accuracy: 0.9900 - val_loss: 0.0201 - val_accuracy: 0.9933\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0004449718.\n",
      "Epoch 19/30\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.0298 - accuracy: 0.9903 - val_loss: 0.0237 - val_accuracy: 0.9933\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.000424869.\n",
      "Epoch 20/30\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.0286 - accuracy: 0.9907 - val_loss: 0.0219 - val_accuracy: 0.9938\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0004065041.\n",
      "Epoch 21/30\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.0290 - accuracy: 0.9906 - val_loss: 0.0213 - val_accuracy: 0.9937\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.000389661.\n",
      "Epoch 22/30\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.0284 - accuracy: 0.9907 - val_loss: 0.0197 - val_accuracy: 0.9940\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0003741581.\n",
      "Epoch 23/30\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.0275 - accuracy: 0.9909 - val_loss: 0.0209 - val_accuracy: 0.9939\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0003598417.\n",
      "Epoch 24/30\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.0280 - accuracy: 0.9907 - val_loss: 0.0216 - val_accuracy: 0.9934\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0003465804.\n",
      "Epoch 25/30\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.0272 - accuracy: 0.9910 - val_loss: 0.0225 - val_accuracy: 0.9933\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0003342618.\n",
      "Epoch 26/30\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.0267 - accuracy: 0.9911 - val_loss: 0.0221 - val_accuracy: 0.9934\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0003227889.\n",
      "Epoch 27/30\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.0269 - accuracy: 0.9912 - val_loss: 0.0221 - val_accuracy: 0.9933\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0003120774.\n",
      "Epoch 28/30\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.0274 - accuracy: 0.9910 - val_loss: 0.0205 - val_accuracy: 0.9939\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.000302054.\n",
      "Epoch 29/30\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 0.0259 - accuracy: 0.9915 - val_loss: 0.0219 - val_accuracy: 0.9940\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0002926544.\n",
      "Epoch 30/30\n",
      "3750/3750 [==============================] - 21s 5ms/step - loss: 0.0266 - accuracy: 0.9918 - val_loss: 0.0218 - val_accuracy: 0.9938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9bc0260550>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are importing the Adam Optimizer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# We are importing the learningratescheduler callback\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "#Creating the \"scheduler\" function with two arguments i.e learningrate and epoch\n",
    "def scheduler(epoch, lr):\n",
    "  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
    "\n",
    "#\tLearningRate = LearningRate * 1/(1 + decay * epoch) here decay is 0.319 and epoch is 10.\n",
    "\n",
    "# here we are compiling our model and using 'categorical_crossentropy' as our loss function and adam as our optimizer with learning rate =0.003 and metrics is accuracy\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr = 0.003), metrics=['accuracy'])\n",
    "\n",
    "# Here we are traing our model using the data and using batch size of 128,number of epochs are 20 and using verbose=1 for printing out all the results.\n",
    "# In the callbacks parameter we are using the LearningRateScheduler which takes two arguments scheduler function which we built earlier to reduce the learning rate in each decay and verbose =1\n",
    "model.fit(X_train, Y_train, batch_size=16, epochs=30, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1789521c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
